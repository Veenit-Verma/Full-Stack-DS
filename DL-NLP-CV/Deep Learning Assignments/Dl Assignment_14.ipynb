{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Solution Assignment : 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tIs it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa27fb",
   "metadata": {},
   "source": [
    "No. All weights should be initialized to different random values and should not have the same initial value. If weights are symmetrical, meaning they have the same value, it makes it almost impossible for backpropagation to converge to a good solution.\n",
    "\n",
    "Think of it this way: if all the weights are the same, it's like having just one neuron per layer, but much slower.\n",
    "\n",
    "The technique we use to break this symmetry is to sample weights randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tIs it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c5b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd73461",
   "metadata": {},
   "source": [
    "Three advantages of the ELU activation function over ReLU:\n",
    "\n",
    "ELU is more robust to vanishing gradients. ReLU activation functions can cause the gradients to vanish when the input is negative, which can make it difficult for the neural network to learn. ELU activation functions are less likely to cause vanishing gradients, which can help the neural network learn more effectively.\n",
    "ELU has a lower probability of dead neurons. ReLU activation functions can cause some neurons to become \"dead\", which means that they never activate. This can happen when the input to a ReLU neuron is always negative. ELU activation functions are less likely to cause dead neurons, which can help the neural network learn more effectively.\n",
    "ELU can learn more complex patterns. ELU activation functions can learn more complex patterns than ReLU activation functions. This is because ELU activation functions are not limited to positive values, which gives them more flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tIn which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b5998",
   "metadata": {},
   "source": [
    "Some cases where you might want to use each of the following activation functions:\n",
    "\n",
    "ELU: ELU activation functions are a good choice for problems where it is important to be robust to vanishing gradients and dead neurons. They can also learn more complex patterns than ReLU activation functions.\n",
    "Leaky ReLU: Leaky ReLU activation functions are a good choice for problems where there is a lot of data with negative values. They are less likely to cause dead neurons than ReLU activation functions, and they can still learn complex patterns.\n",
    "ReLU: ReLU activation functions are a good choice for problems where it is important to be computationally efficient. They are also a good choice for problems where the data is mostly positive.\n",
    "tanh: tanh activation functions are a good choice for problems where it is important to have a symmetric output range. They are also a good choice for problems where the data is mostly real-valued.\n",
    "Logistic: Logistic activation functions are a good choice for problems where the output should be a probability. They are also a good choice for problems where the data is mostly binary.\n",
    "Softmax: Softmax activation functions are a good choice for problems where the output should be a probability distribution. They are also a good choice for problems where the data is mostly categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64c74e",
   "metadata": {},
   "source": [
    "If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer, the optimizer may become too persistent and slow to converge. This is because the momentum term will have a lot of weight, and it will be difficult for the optimizer to move away from its current position.\n",
    "\n",
    "Here is an explanation of how momentum works:\n",
    "\n",
    "Momentum: Momentum is a technique used to accelerate the learning process in gradient descent. It does this by storing a running average of the gradients and using that average to update the weights.\n",
    "Hyperparameter: A hyperparameter is a parameter that is used to control the learning process. The momentum hyperparameter controls how much weight is given to the running average of the gradients.\n",
    "When the momentum hyperparameter is set to 1, the optimizer will only update the weights in the direction of the current gradient. This means that the optimizer will be very persistent and will not be able to move away from its current position.\n",
    "\n",
    "If you are using a MomentumOptimizer, it is important to set the momentum hyperparameter to a value that is not too close to 1. A good value for the momentum hyperparameter is typically between 0.9 and 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e57d6",
   "metadata": {},
   "source": [
    "Three ways you can produce a sparse model:\n",
    "\n",
    "Use a regularization technique such as L1 or L2 regularization. These techniques penalize the size of the weights, which can help to encourage the model to use fewer weights.\n",
    "Use a pruning technique. Pruning techniques remove weights from the model that are not very important. This can be done manually or automatically.\n",
    "Use a data-driven approach. This involves using data to select the weights that are most important for the model. This can be done using a technique called feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f34d2e",
   "metadata": {},
   "source": [
    "dropout can slow down training. This is because dropout randomly drops out neurons during training, which means that the model has to be trained multiple times to account for all possible combinations of neurons that are dropped out.\n",
    "\n",
    "However, dropout can also speed up inference. This is because dropout forces the model to learn to rely on multiple neurons, which can make the model more robust to noise and less likely to overfit.\n",
    "Start with a low dropout rate: When you are first starting out, it is a good idea to start with a low dropout rate. This will help to prevent the model from becoming too unstable.\n",
    "Increase the dropout rate gradually: Once you have a good understanding of how dropout works, you can start to increase the dropout rate gradually. This will help you to find the optimal dropout rate for your specific problem.\n",
    "Experiment with different dropout rates: The best way to find the optimal dropout rate is to experiment with different dropout rates. This will help you to find the dropout rate that works best for your specific problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
