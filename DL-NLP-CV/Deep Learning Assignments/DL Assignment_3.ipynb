{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Solution: Assignment 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b6bd5",
   "metadata": {},
   "source": [
    "Problems that can occur if you initialize all the weights to the same value:\n",
    "\n",
    "-The network will not be able to learn anything.\n",
    "\n",
    "-The network will be very sensitive to the initial weights, and small changes in the weights can have a large impact on the performance of the network.\n",
    "\n",
    "-The network will be more likely to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tIs it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4871b6",
   "metadata": {},
   "source": [
    "We can initialize the bias terms to 0 and this is a common practice. The bias terms are a vector of numbers that are added to the output of each neuron in the network. Initializing the bias terms to 0 will ensure that all the neurons in the network start out with the same output. This can help the network to learn more effectively, as it will not be biased towards any particular output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10819c3",
   "metadata": {},
   "source": [
    "\n",
    "Three advantages of the SELU activation function over ReLU:\n",
    "\n",
    "-Self-normalizing: SELU activation function has a property called self-normalization, which means that it automatically normalizes the input to the next layer, making it easier for the network to learn. This is in contrast to ReLU, which can suffer from the vanishing gradient problem, where the gradients become very small and the network stops learning.\n",
    "\n",
    "-Faster training: SELU activation function has been shown to train faster than ReLU, even on deep networks. This is because the self-normalization property of SELU helps to stabilize the training process and prevent the network from getting stuck in local minima.\n",
    "\n",
    "-Better performance: SELU activation function has been shown to achieve better performance than ReLU on a variety of tasks, including image classification, natural language processing, and speech recognition. This is likely due to the fact that SELU is better at preserving the information in the input, which allows the network to learn more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdf462",
   "metadata": {},
   "source": [
    "SELU: SELU is a good choice for general-purpose use. It is self-normalizing, which means that it automatically normalizes the input to the next layer, making it easier for the network to learn. It is also faster to train than ReLU, and it has been shown to achieve better performance on a variety of tasks.\n",
    "Leaky ReLU: Leaky ReLU is a variant of ReLU that allows for small negative values to pass through the activation function. This can help to prevent the vanishing gradient problem, which can occur with ReLU. Leaky ReLU is a good choice for tasks where the input data is likely to contain negative values.\n",
    "ReLU: ReLU is a simple and effective activation function that is often used as a default choice. It is not self-normalizing, but it is relatively fast to train and it can achieve good performance on a variety of tasks.\n",
    "Tanh: Tanh is a non-linear activation function that is often used for regression tasks. It has a range of [-1, 1], which makes it well-suited for tasks where the output should be bounded.\n",
    "Logistic: Logistic is a non-linear activation function that is often used for classification tasks. It has a range of [0, 1], which makes it well-suited for tasks where the output should be a probability.\n",
    "Softmax: Softmax is a non-linear activation function that is often used for multi-class classification tasks. It normalizes the output of the network, so that the sum of the outputs is equal to 1. This makes it possible to interpret the output of the network as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba843b24",
   "metadata": {},
   "source": [
    "If we set the momentum hyperparameter too close to 1 when using an SGD optimizer, then the algorithm may oscillate around the minimum instead of converging to it. This is because the momentum term will keep the weights moving in the same direction, even if the gradient is pointing in the opposite direction. This can lead to the algorithm getting stuck in a local minimum. Also, setting the momentum hyperparameter too close to 1 can also make the training process more unstable. This is because the momentum term can amplify the noise in the gradients, which can lead to the algorithm overshooting the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb43d13",
   "metadata": {},
   "source": [
    "Three ways to produce a sparse model:\n",
    "\n",
    "-Lasso regularization: Lasso regularization is a technique that penalizes the model for having large coefficients. This encourages the model to use only the most important features, which can result in a sparse model.\n",
    "\n",
    "-Ridge regularization: Ridge regularization is another technique that penalizes the model for having large coefficients. However, ridge regularization does not encourage sparsity as much as lasso regularization.\n",
    "\n",
    "-Early stopping: Early stopping is a technique that stops the training of the model early, before it has fully converged. \n",
    "This can help to prevent the model from overfitting the training data, which can result in a sparse model.\n",
    "\n",
    "In addition to these three methods, there are other techniques that can be used to produce a sparse model. These techniques include:\n",
    "\n",
    "-Feature selection: Feature selection is the process of selecting the most important features for the model. This can help to produce a sparse model by removing features that are not important.\n",
    "\n",
    "-Data preprocessing: Data preprocessing techniques such as normalization and feature scaling can help to produce a sparse model by reducing the variance of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf2a47",
   "metadata": {},
   "source": [
    "Dropout can slow down training. This is because dropout randomly drops out a certain percentage of neurons during training, which means that the network has to be trained multiple times to account for all possible configurations of neurons. This can add significant overhead to the training process.\n",
    "\n",
    "Dropout does not slow down inference. This is because dropout is only applied during training, and it is not applied when the network is making predictions on new instances.\n",
    "\n",
    "MC dropout is a variant of dropout that uses multiple dropout masks during training. This can help to improve the generalization performance of the network, but it can also slow down training.\n",
    "\n",
    "In general, the impact of dropout on training and inference speed depends on the following factors:\n",
    "\n",
    "The dropout rate: The higher the dropout rate, the more the training will be slowed down.\n",
    "The size of the network: Larger networks are more likely to be slowed down by dropout than smaller networks.\n",
    "The complexity of the task: More complex tasks are more likely to be slowed down by dropout than simpler tasks.\n",
    "If you are concerned about the impact of dropout on training and inference speed, you can try using a lower dropout rate or a smaller network. You can also try using MC dropout, which can help to improve generalization performance without significantly slowing down training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tPractice training a deep neural network on the CIFAR10 image dataset:\n",
    "1.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "2.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "3.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "4.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "5.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256c5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
