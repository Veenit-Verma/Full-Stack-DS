{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Solution: Assignment 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1. Explain the Activation Functions in your own language\n",
    "- a) sigmoid\n",
    "- b) tanh\n",
    "- c) ReLU\n",
    "- d) ELU\n",
    "- e) LeakyReLU\n",
    "- f) swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef6001",
   "metadata": {},
   "source": [
    "Sigmoid\n",
    "\n",
    "The sigmoid function is a non-linear function that has a sigmoid shape. It is often used in classification problems, as it can output values between 0 and 1, which can be interpreted as probabilities. The sigmoid function is defined as:\n",
    "\n",
    "f(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "Tanh\n",
    "\n",
    "The tanh function is similar to the sigmoid function, but it has a range of [-1, 1]. This makes it a good choice for applications where the output needs to be bounded, such as in regression problems. The tanh function is defined as:\n",
    "\n",
    "f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "ReLU\n",
    "\n",
    "The ReLU function is a non-linear function that is often used in deep learning. It is defined as:\n",
    "\n",
    "f(x) = max(0, x)\n",
    "The ReLU function has a few advantages over other activation functions, such as the sigmoid and tanh functions. First, it is computationally efficient. Second, it does not saturate, which means that it can learn to represent a wider range of values. Third, it is less prone to vanishing gradients, which is a problem that can occur with other activation functions.\n",
    "\n",
    "ELU\n",
    "\n",
    "The ELU function is a non-linear function that is similar to the ReLU function. However, the ELU function has a negative slope for negative inputs, which can help to improve the performance of neural networks on certain tasks. The ELU function is defined as:\n",
    "\n",
    "f(x) = x * (1 + exp(-x))\n",
    "\n",
    "LeakyReLU\n",
    "\n",
    "The LeakyReLU function is a variant of the ReLU function that has a small slope for negative inputs. This helps to prevent the vanishing gradient problem that can occur with the ReLU function. The LeakyReLU function is defined as:\n",
    "\n",
    "f(x) = max(0.01x, x)\n",
    "\n",
    "Swish\n",
    "\n",
    "The Swish function is a non-linear function that is a recent addition to the list of activation functions. The Swish function is defined as:\n",
    "\n",
    "f(x) = x * sigmoid(x)\n",
    "The Swish function has been shown to be effective in a variety of tasks, including image classification and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2. What happens when you increase or decrease the optimizer learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f456c8e",
   "metadata": {},
   "source": [
    "The learning rate is a hyperparameter that controls how much the model weights are updated during training. A higher learning rate means that the weights will be updated more aggressively, while a lower learning rate means that the weights will be updated more slowly.\n",
    "\n",
    "Increasing the learning rate can make the model train faster, but it can also make the model more likely to overfit. Overfitting occurs when the model learns the training data too well and is not able to generalize to new data. Decreasing the learning rate can make the model train slower, but it can also make the model less likely to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3. What happens when you increase the number of internal hidden neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82000feb",
   "metadata": {},
   "source": [
    "Increasing the number of internal hidden neurons in a neural network can have a number of effects, including:\n",
    "\n",
    "Increased model complexity: A neural network with more hidden neurons is more complex than a neural network with fewer hidden neurons. This means that the model may take longer to train and may be more difficult to interpret.\n",
    "Improved model performance: A neural network with more hidden neurons may be able to learn more complex patterns in the data, which can lead to improved model performance.\n",
    "Increased risk of overfitting: A neural network with more hidden neurons is more likely to overfit the training data. This means that the model may perform well on the training data but may not perform well on new data.\n",
    "The optimal number of hidden neurons for a particular neural network will depend on the specific problem and the hyperparameters that are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4. What happens when you increase the size of batch computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff368d5c",
   "metadata": {},
   "source": [
    "Increasing the size of batch computation has 2 main effects:\n",
    "\n",
    "Increased training speed: A larger batch size means that the model can be updated more frequently, which can lead to faster training.\n",
    "Decreased training stability: A larger batch size can make the training process less stable, which means that the model may be more likely to diverge.\n",
    "The optimal batch size for a particular model will depend on the specific problem and the hyperparameters that are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5. Why we adopt regularization to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2f696",
   "metadata": {},
   "source": [
    "Some of the benefits of using regularization to avoid overfitting in deep learning:\n",
    "\n",
    "Improved model accuracy on new data\n",
    "Reduced variance of the model\n",
    "Increased interpretability of the model\n",
    "Regularization is a powerful technique that can be used to improve the performance of deep learning models. It is a valuable tool for preventing overfitting and improving the overall quality of the model.\n",
    "\n",
    "In addition to L1 and L2 regularization, there are other regularization techniques that can be used in deep learning, such as dropout and data augmentation. Dropout randomly drops out neurons during training, which helps to prevent the model from becoming too dependent on any particular set of features. Data augmentation artificially expands the training data by creating new data samples from existing data samples. This helps to prevent the model from overfitting to the specific features of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6. What are loss and cost functions in deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81add876",
   "metadata": {},
   "source": [
    "In deep learning, a loss function is a measure of how well the model's predictions match the ground truth labels. The cost function is the average of the loss functions over all the data samples. The goal of deep learning is to minimize the cost function, which means finding the parameters of the model that minimize the error between the predictions and the ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7. What do ou mean by underfitting in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9276f",
   "metadata": {},
   "source": [
    "Underfitting is a problem that occurs when a neural network is not able to learn the underlying patterns in the data. This can happen if the network is too simple, or if the training data is not representative of the data that the network will be used on.\n",
    "\n",
    "Underfitting can be characterized by the following:\n",
    "\n",
    "The model has a high training error.\n",
    "The model does not generalize well to new data.\n",
    "The model is not able to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8. Why we use Dropout in Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87781ecf",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique that is used to prevent neural networks from overfitting. Overfitting occurs when a neural network learns the training data too well and as a result, does not generalize well to new data. This means that the model will perform well on the training data, but it will not be able to make accurate predictions on new data.\n",
    "\n",
    "Dropout works by randomly dropping out neurons during training. This means that some of the neurons in the network will not be used to calculate the output of the network. This forces the network to learn to rely on all of the neurons, and it prevents the network from becoming too dependent on any particular set of neurons.\n",
    "\n",
    "Dropout has been shown to be an effective way to prevent overfitting in neural networks. It has been used successfully in a variety of applications, including image classification, natural language processing, and speech recognition.\n",
    "\n",
    "Here are some of the benefits of using dropout in neural networks:\n",
    "\n",
    "Prevents overfitting: Dropout helps to prevent overfitting by forcing the network to learn to rely on all of the neurons. This makes the network less likely to memorize the training data and more likely to generalize well to new data.\n",
    "Improves generalization: Dropout can help to improve the generalization performance of neural networks by making them less sensitive to noise in the training data.\n",
    "Increases robustness: Dropout can help to increase the robustness of neural networks by making them less sensitive to the dropout rate. This means that the network will still be able to perform well even if some of the neurons are dropped out during testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
