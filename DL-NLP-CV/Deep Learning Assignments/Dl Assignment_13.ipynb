{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Solution Assignment : 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tWhy is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f9cb4",
   "metadata": {},
   "source": [
    "Generally preferable to use a Logistic Regression classifier rather than a classical Perceptron.\n",
    "\n",
    "Logistic Regression is more versatile. A Perceptron can only be used to classify data into two classes, while Logistic Regression can be used to classify data into any number of classes.\n",
    "Logistic Regression is more robust to noise. A Perceptron is sensitive to noise in the data, while Logistic Regression is more robust to noise.\n",
    "Logistic Regression is easier to train. The Perceptron training algorithm can be difficult to converge, while Logistic Regression can be trained using a variety of optimization algorithms.\n",
    "To tweak a Perceptron to make it equivalent to a Logistic Regression classifier, you can use the following steps:\n",
    "\n",
    "Add a sigmoid activation function to the output layer of the Perceptron.\n",
    "Use a cross-entropy loss function instead of the Perceptron loss function.\n",
    "Train the Perceptron using an optimization algorithm that is designed for Logistic Regression, such as stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tWhy was the logistic activation function a key ingredient in training the first MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d2275",
   "metadata": {},
   "source": [
    "The logistic activation function was a key ingredient in training the first MLPs because it allowed the MLPs to learn non-linear decision boundaries.\n",
    "\n",
    "A linear decision boundary is a line that separates two classes of data. For example, a linear decision boundary could be used to separate cats from dogs. However, many real-world problems require more complex decision boundaries than linear decision boundaries. For example, the decision boundary between cats and dogs might be non-linear, such as a circle.\n",
    "\n",
    "The logistic activation function is a non-linear function that can be used to learn non-linear decision boundaries. The logistic activation function takes a real number as input and outputs a number between 0 and 1. The output of the logistic activation function is a probability that the input belongs to the positive class.\n",
    "\n",
    "The logistic activation function was a key ingredient in training the first MLPs because it allowed the MLPs to learn non-linear decision boundaries. This made it possible for the MLPs to solve problems that could not be solved with linear classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f3270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tSuppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "- What is the shape of the input matrix X?\n",
    "- What about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\n",
    "- What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    "- What is the shape of the network’s output matrix Y?\n",
    "- Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07e640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tHow many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ff4dc",
   "metadata": {},
   "source": [
    "If you want to classify email into spam or ham, you need one neuron in the output layer. This is because there are two classes of email: spam and ham. The activation function that you should use in the output layer is the sigmoid activation function. The sigmoid activation function takes a real number as input and outputs a number between 0 and 1. The output of the sigmoid activation function is a probability that the input belongs to the positive class. In this case, the positive class is spam.\n",
    "\n",
    "If you want to tackle MNIST, you need 10 neurons in the output layer. This is because there are 10 classes of handwritten digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. The activation function that you should use in the output layer is the softmax activation function. The softmax activation function takes a vector of real numbers as input and outputs a vector of probabilities. The sum of the probabilities in the output vector is 1. In this case, the output vector represents the probability that the input image belongs to each of the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tWhat is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201557d",
   "metadata": {},
   "source": [
    "Backpropagation is an algorithm for computing the gradients of a loss function with respect to the parameters of a neural network. It is a key part of the training process for neural networks.\n",
    "\n",
    "Backpropagation works by starting at the output layer of the neural network and propagating the gradients backwards through the network. The gradients are computed using the chain rule, which is a mathematical identity that allows us to compute the derivative of a composite function.\n",
    "\n",
    "The gradients are used to update the parameters of the neural network using an optimization algorithm, such as stochastic gradient descent. This process is repeated until the neural network converges to a solution.\n",
    "\n",
    "Reverse-mode autodiff is a technique for computing the gradients of a function using automatic differentiation. Automatic differentiation is a general technique for computing the derivatives of functions without having to explicitly write out the derivatives.\n",
    "\n",
    "Reverse-mode autodiff works by starting at the output of the function and propagating the derivatives backwards through the function. The derivatives are computed using the chain rule, just like backpropagation.\n",
    "\n",
    "The main difference between backpropagation and reverse-mode autodiff is that backpropagation is specifically designed for neural networks, while reverse-mode autodiff is a general technique that can be used to compute the gradients of any function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tCan you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446a48d",
   "metadata": {},
   "source": [
    "Some of the hyperparameters you can tweak in an MLP:\n",
    "\n",
    "Number of layers: The number of layers in an MLP determines the complexity of the model. A model with more layers can learn more complex patterns, but it can also be more prone to overfitting.\n",
    "Number of neurons per layer: The number of neurons per layer determines the number of features that the model can learn. A model with more neurons can learn more features, but it can also be more prone to overfitting.\n",
    "Activation function: The activation function determines how the neurons in the model process information. Different activation functions can lead to different learning behaviors.\n",
    "Learning rate: The learning rate determines how quickly the model learns. A higher learning rate can lead to faster learning, but it can also lead to overfitting.\n",
    "Batch size: The batch size determines how many data points are processed at a time. A larger batch size can lead to more stable learning, but it can also be more computationally expensive.\n",
    "Epochs: The number of epochs determines how many times the model is trained on the data. A higher number of epochs can lead to better performance, but it can also take longer to train the model.\n",
    "Regularization: Regularization is a technique for preventing overfitting. There are different types of regularization, such as L1 regularization and L2 regularization.\n",
    "If the MLP overfits the training data, you can try to tweak these hyperparameters to try to solve the problem:\n",
    "\n",
    "Reduce the number of layers. A model with fewer layers is less likely to overfit.\n",
    "Reduce the number of neurons per layer. A model with fewer neurons is less likely to overfit.\n",
    "Change the activation function. A different activation function can lead to different learning behaviors, which can help to prevent overfitting.\n",
    "Reduce the learning rate. A lower learning rate can help to prevent overfitting.\n",
    "Use a smaller batch size. A smaller batch size can help to prevent overfitting.\n",
    "Increase the number of epochs. A larger number of epochs can help to prevent overfitting.\n",
    "Use regularization. Regularization can help to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tTrain a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e9098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
