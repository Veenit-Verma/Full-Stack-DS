{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Solution Assignment : 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tHow does unsqueeze help us to solve certain broadcasting problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133cbcb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tHow can we use indexing to do the same operation as unsqueeze?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e204d399",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tHow do we show the actual contents of the memory used for a tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1b727",
   "metadata": {},
   "source": [
    "To show the actual contents of the memory used for a tensor, you can use the .data attribute of the tensor.If you want to see the actual contents of the memory used by the tensor, you can use the print() function to print the value of the .data attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tWhen adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7ef2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tDo broadcasting and expand_as result in increased memory use? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608b032",
   "metadata": {},
   "source": [
    "Broadcasting and expand_as can result in increased memory use, depending on the specific situation.\n",
    "\n",
    "Broadcasting is a way of performing arithmetic operations between arrays of different shapes. When broadcasting occurs, the smaller array is \"broadcasted\" to match the shape of the larger array. This is done by adding \"phantom\" dimensions of size 1 to the smaller array.\n",
    "\n",
    "The phantom dimensions do not contain any data, but they do take up space in memory. Therefore, broadcasting can result in increased memory use, especially if the smaller array is large.\n",
    "\n",
    "Expand_as is a method that can be used to match the shapes of two arrays so that they can be used in an arithmetic operation. When expand_as is used, a new array is created that has the same shape as the input array.\n",
    "\n",
    "The new array is created by copying the data from the input array. Therefore, expand_as can also result in increased memory use, especially if the input array is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tImplement matmul using Einstein summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d2239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tWhat does a repeated index letter represent on the lefthand side of einsum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf2ab8",
   "metadata": {},
   "source": [
    "In Einstein summation notation, a repeated index letter on the left-hand side of an einsum contraction represents summation over that index.an einsum contraction with a repeated index letter on the left-hand side is equivalent to a for loop that sums over the index. The number of times that the index is summed over is equal to the number of times that the index appears in the subscript.\n",
    "\n",
    "Examples of how repeated index letters are used in Einstein summation notation:\n",
    "\n",
    "einsum('ij,jk->ik', A, B) is equivalent to summing over the index j in the product of the matrices A and B.\n",
    "einsum('ijk,jkl->ijl', A, B) is equivalent to summing over the index k in the product of the matrices A and B.\n",
    "einsum('ijk,k->ij', A, B) is equivalent to summing over the index k in the product of the matrices A and B, and then removing the index k from the output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tWhat are the three rules of Einstein summation notation? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e9954",
   "metadata": {},
   "source": [
    "Three rules of Einstein summation notation:\n",
    "\n",
    "Repeated indices are summed over. This means that if an index appears twice in a subscript, it is summed over all possible values of that index. For example, the subscript ij in the einsum contraction einsum('ij,jk->ik', A, B) means that the indices i and j are summed over.\n",
    "The number of times an index is summed over is equal to the number of times it appears in the subscript. This means that if an index appears three times in a subscript, it is summed over three times. For example, the subscript ijk in the einsum contraction einsum('ijk,jkl->ijl', A, B) means that the index k is summed over three times.\n",
    "The order of the indices in the subscript is irrelevant. This means that the subscript ij is the same as the subscript ji. For example, the einsum contraction einsum('ij,jk->ik', A, B) is the same as the einsum contraction einsum('ji,kj->ik', A, B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3af2",
   "metadata": {},
   "source": [
    "#### 9.\tWhat are the forward pass and backward pass of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374f82c",
   "metadata": {},
   "source": [
    "The forward pass is the process of computing the output of a neural network given an input. The backward pass is the process of computing the gradients of the loss function with respect to the parameters of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bd31",
   "metadata": {},
   "source": [
    "#### 10.\tWhy do we need to store some of the activations calculated for intermediate layers in the forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e2ab7",
   "metadata": {},
   "source": [
    "We need to store some of the activations calculated for intermediate layers in the forward pass because they are needed to compute the gradients during backpropagation.\n",
    "\n",
    "During backpropagation, the gradients are computed layer-by-layer, starting from the output layer and working back to the input layer. The gradients for a given layer are computed using the activations of the previous layer.\n",
    "\n",
    "If we do not store the activations of the intermediate layers, then we would not be able to compute the gradients for those layers. This would make it impossible to update the parameters of the neural network, and the neural network would not be able to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860f8b5",
   "metadata": {},
   "source": [
    "#### 11.\tWhat is the downside of having activations with a standard deviation too far away from 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b7b3c",
   "metadata": {},
   "source": [
    "Activations with a standard deviation too far away from 1 can have a number of downsides, including:\n",
    "\n",
    "Reduced learning rate stability: When the standard deviation of activations is too high, the gradients can become very large, which can make it difficult for the neural network to learn. This is because the gradients will be more likely to \"overshoot\" the optimal weights, which can lead to oscillations in the training process.\n",
    "Increased sensitivity to initialization: When the standard deviation of activations is too high, the neural network becomes more sensitive to the initial weights. This is because the gradients will be more likely to be amplified by the large standard deviation, which can lead to the neural network getting stuck in local minima.\n",
    "Reduced generalization performance: When the standard deviation of activations is too high, the neural network may learn to fit the training data too well, which can lead to poor generalization performance on new data. This is because the large standard deviation can cause the neural network to learn features that are specific to the training data, which will not be present in new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c713869",
   "metadata": {},
   "source": [
    "#### 12.\tHow can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5509cc1",
   "metadata": {},
   "source": [
    "One way to avoid the problem of activations with a standard deviation too far away from 1 is to use a Xavier initialization. Xavier initialization is a popular method of weight initialization that ensures that the standard deviation of activations is close to 1.\n",
    "\n",
    "Xavier initialization works by initializing the weights of a neural network with values that are drawn from a uniform distribution. The standard deviation of this distribution is chosen so that the expected value of the activations is 0 and the standard deviation of the activations is 1.\n",
    "\n",
    "Xavier initialization helps to avoid the problem of activations with a standard deviation too far away from 1 by ensuring that the weights are initialized in a way that does not amplify the gradients. This helps to ensure that the neural network learns in a stable and generalizable way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
