{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8dc85bb",
   "metadata": {},
   "source": [
    "1.Describe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?\n",
    "\n",
    "The structure of an artificial neuron is similar to that of a biological neuron in several ways. Both artificial and biological neurons have inputs, weights, and a bias. Both types of neurons also have an activation function that determines whether the neuron will be activated or not. However, there are also some important differences between artificial and biological neurons. Artificial neurons are typically much simpler than biological neurons. They typically have fewer inputs, and their weights and bias are typically not as complex. Additionally, the activation functions used in artificial neurons are typically simpler than the activation functions used in biological neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa935cbd",
   "metadata": {},
   "source": [
    "2.What are the different types of activation functions popularly used? Explain each of them.\n",
    "\n",
    "Activation functions used in neural networks:\n",
    "\n",
    "a. Sigmoid function: The sigmoid function is a non-linear function that has a sigmoid shape. The sigmoid function is typically used in the output layer of a neural network to produce a probability between 0 and 1.\n",
    "b. Tanh function: The tanh function is similar to the sigmoid function, but it has a range of -1 to 1. The tanh function is typically used in the hidden layers of a neural network to introduce non-linearity.\n",
    "c. ReLU function: The ReLU function is a non-linear function that has a linear shape for positive inputs and a zero output for negative inputs. The ReLU function is a popular activation function because it is computationally efficient and it can help to prevent neural networks from overfitting the training data.\n",
    "d. Leaky ReLU function: The Leaky ReLU function is a variant of the ReLU function that has a small slope for negative inputs. The Leaky ReLU function is designed to address the vanishing gradient problem that can occur with the ReLU function.\n",
    "e. ELU function: The ELU function is another variant of the ReLU function that has a more gradual slope for negative inputs. The ELU function is designed to improve the performance of neural networks on some tasks.\n",
    "f. Softmax function: The softmax function is a non-linear function that is typically used in the output layer of a neural network to produce a probability distribution over the output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95712085",
   "metadata": {},
   "source": [
    "3. Write the short note: \n",
    "    1.Explain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?\n",
    "    2.Use a simple perceptron with weights w0, w1, and w2 as −1, 2, and 1, respectively, to classify \n",
    "      data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095e973",
   "metadata": {},
   "source": [
    "4. Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f99407",
   "metadata": {},
   "source": [
    "5. What is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN.\n",
    "\n",
    " ANNs are made up of artificial neurons, that work together to learn from data.\n",
    "Architectural options for ANNs:\n",
    "Feedforward ANNs: Feedforward ANNs have a unidirectional flow of information, from the input layer to the output layer.\n",
    "Recurrent ANNs: Recurrent ANNs have feedback loops, which allow information to flow back to earlier layers. This makes them well-suited for tasks that involve sequential data, such as natural language processing and speech recognition.\n",
    "Convolutional ANNs: Convolutional ANNs are a type of feedforward ANN that uses convolution operations to extract features from data. This makes them well-suited for tasks that involve images, such as image classification and object detection.\n",
    "Deep ANNs: Deep ANNs are ANNs with multiple layers of artificial neurons. This allows them to learn more complex relationships between the features of the data.\n",
    "\n",
    "Some of the salient highlights in the different architectural options for ANNs:\n",
    "Feedforward ANNs: Feedforward ANNs are simple and easy to understand. They are also relatively efficient to train. However, they can be limited in their ability to learn complex relationships between the features of the data.\n",
    "Recurrent ANNs: Recurrent ANNs are more complex than feedforward ANNs, but they can learn more complex relationships between the features of the data. They are also well-suited for tasks that involve sequential data. However, they can be more difficult to train than feedforward ANNs.\n",
    "Convolutional ANNs: Convolutional ANNs are a type of feedforward ANN that is specifically designed for processing images. They are able to extract features from images that are invariant to translation, rotation, and scale. This makes them well-suited for tasks such as image classification and object detection.\n",
    "Deep ANNs: Deep ANNs are able to learn more complex relationships between the features of the data than shallow ANNs. This makes them well-suited for tasks that require a high degree of accuracy, such as image recognition and natural language processing. However, they can be more difficult to train than shallow ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934817da",
   "metadata": {},
   "source": [
    "6. Explain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?\n",
    "\n",
    "Learning process in ANN mainly depends on four factors, they are:\n",
    "\n",
    "a. The number of layers in the network (Single-layered or multi-layered)\n",
    "b. Direction of signal flow (Feedforward or recurrent)\n",
    "c. Number of nodes in layers: The number of node in the input layer is equal to the number of features of the input data    set. The number of output nodes will depend on possible outcomes i.e. the number of classes in case of supervised learning. But the number of layers in the hidden layer is to be chosen by the user. A larger number of nodes in the hidden layer, higher the performance but too many nodes may result in overfitting as well as increased computational expense.\n",
    "d. Weight of Interconnected Nodes: Deciding the value of weights attached with each interconnection between each neuron so  that a specific learning problem can be solved correctly is quite a difficult problem by itself. \n",
    "\n",
    "Take an example to understand the problem where assigning synapic weights is an issue.. Take the example of a Multi-layered Feed-Forward Network, we have to train an ANN model using some data, so that it can classify a new data set, say p_5(3,-2). Say we have deduced that p_1=(5,2)   and  p_2 = (-1,12)   belonging to class C1 while p_3=(3,-5)   and p_4 = (-2,-1)  belonging to class C2. We assume the values of synaptic weights w_0,w_1,w_2 as -2, 1/2 and 1/4 respectively. But we will NOT get these weight values for every learning problem. For solving a learning problem with ANN, we can start with a set of values for synaptic weights and keep changing those in multiple iterations. The stopping criterion may be the rate of misclassification < 1% or the maximum numbers of iterations should be less than 25(a threshold value). There may be another problem that, the rate of misclassification may not reduce progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b5029",
   "metadata": {},
   "source": [
    "7. Explain, in details, the backpropagation algorithm. What are the limitations of this algorithm?\n",
    "\n",
    "Backpropagation is an algorithm that uses gradient descent for supervised learning of artificial neural networks. Short for backward propagation of errors, it calculates the gradient of the error function with regard to the neural network's weights.The gradient of the final layer of weights is calculated first in backpropagation, and the gradient of the first layer of weights is calculated last. The calculation of e gradient of error essentially occurs backwards through the artificial neural network.The backpropagation algorithm goes back from the results obtained and rectifies its errors at every node of the neural network so as to improve the performance of that neural network model. \n",
    "It reuses partial computations of the gradient of a layer while calculating the gradient of the next layer.\n",
    "\n",
    "Limitations of the Backpropagation algorithm:\n",
    "\n",
    "It is slow, all previous layers are locked until gradients for the current layer is calculated\n",
    "It suffers from vanishing or exploding gradients problem\n",
    "It suffers from overfitting & underfitting problem\n",
    "It considers predicted value & actual value only to calculate error and to calculate gradients, related to the objective function, partially related to the Backpropagation algorithm\n",
    "It doesn’t consider the spatial, associative and dis-associative relationship between classes while calculating errors, related to the objective function, partially related to the Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cda1a9",
   "metadata": {},
   "source": [
    "8.Describe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0bd0e",
   "metadata": {},
   "source": [
    "9.What are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86fcf06",
   "metadata": {},
   "source": [
    "10. Write short notes on:\n",
    "    1.Artificial neuron\n",
    "    2.Multi-layer perceptron\n",
    "    3.Deep learning\n",
    "    4.Learning rate\n",
    "\n",
    "Artificial neuron: An artificial neuron is a mathematical model that is inspired by biological neurons. An artificial neuron has a number of inputs, and it outputs a single value. The output of an artificial neuron is determined by the weights of the inputs, and by an activation function.\n",
    "\n",
    "Multi-layer perceptron: A multi-layer perceptron is a neural network that has multiple layers of artificial neurons. The neurons in a multi-layer perceptron are connected to each other, and they are trained to learn a specific task. Multi-layer perceptrons are often used for classification tasks, such as image classification and natural language processing.\n",
    "\n",
    "Deep learning: Deep learning is a type of machine learning that uses artificial neural networks with multiple layers. Deep learning algorithms are able to learn complex relationships between the inputs and outputs of a neural network, and they are often used for tasks that are difficult or impossible for traditional machine learning algorithms to solve.\n",
    "\n",
    "Learning rate: The learning rate is a hyperparameter that controls how much the weights of a neural network are updated during training. A high learning rate means that the weights are updated more quickly, while a low learning rate means that the weights are updated more slowly. The learning rate is a critical hyperparameter, and it needs to be tuned carefully in order to achieve good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3144a7",
   "metadata": {},
   "source": [
    "11. Write the difference between:-\n",
    "    1.Activation function vs threshold function\n",
    "    2.Step function vs sigmoid function\n",
    "    3.Single layer vs multi-layer perceptron\n",
    "    \n",
    "    1.Activation function vs threshold function:\n",
    "   - Activation Function: An activation function is a mathematical function applied to the output of a neuron in a neural network. It introduces non-linearity to the network, enabling it to learn complex patterns and make continuous predictions. Activation functions are typically differentiable, which allows for the use of gradient-based optimization algorithms during training. Examples of activation functions include sigmoid, tanh, ReLU, and softmax.\n",
    "   - Threshold Function: A threshold function, also known as a step function, is a simple activation function that maps inputs to binary outputs based on a predefined threshold. It outputs a value of 1 if the input exceeds the threshold and 0 otherwise. The threshold function does not allow for smooth transitions or intermediate values between the outputs. It is commonly used in binary classification tasks.\n",
    "\n",
    "     2. Step function vs sigmoid function:\n",
    "   - Step Function: The step function, also known as the Heaviside step function, is a simple threshold function that outputs binary values. It returns a value of 0 for inputs below the threshold and 1 for inputs equal to or above the threshold. It has a discontinuous nature and does not allow for smooth transitions or intermediate values.\n",
    "   - Sigmoid Function: The sigmoid function is a smooth, non-linear activation function that maps the input to a value between 0 and 1. It has an S-shaped curve, and as the input approaches positive or negative infinity, the output approaches 1 or 0, respectively. The sigmoid function is differentiable, making it suitable for gradient-based optimization algorithms. It is commonly used in feedforward neural networks for binary classification tasks and as an activation function in hidden layers.\n",
    "\n",
    "     3. Single-layer perceptron vs multi-layer perceptron:\n",
    "   - Single-layer Perceptron: A single-layer perceptron is the simplest form of a neural network. It consists of only one layer of neurons, including the input layer and the output layer. It does not have any hidden layers. The single-layer perceptron is limited to solving linearly separable problems and performs binary classification tasks. It uses a linear activation function, such as the step or threshold function, to make decisions based on a linear combination of inputs.\n",
    "   - Multi-layer Perceptron: A multi-layer perceptron (MLP) is a more advanced neural network architecture with one or more hidden layers between the input and output layers. The hidden layers contain additional neurons that introduce non-linearity, allowing the network to learn complex patterns. MLPs can solve nonlinear problems and perform tasks such as regression, classification, and even complex decision-making. They use non-linear activation functions, such as sigmoid, tanh, or ReLU, to process and propagate information through the layers. Backpropagation is commonly used to train multi-layer perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee40b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
